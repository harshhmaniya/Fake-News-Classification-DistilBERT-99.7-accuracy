# ğŸ“° Fake News Classification Using DistilBERT ğŸš€

This project fine-tunes the **DistilBERT** model to classify news articles as **real** or **fake**, achieving an impressive **99.7% accuracy**. The process involves **data loading, tokenization, model training, evaluation, and prediction**.

---

## ğŸ“Œ Table of Contents

- [ğŸ“– Overview](#-overview)
- [ğŸ› ï¸ Setup & Installation](#%EF%B8%8F-setup--installation)
- [ğŸ“‚ Dataset](#-dataset)
- [âš™ï¸ Model Training Process](#%EF%B8%8F-model-training-process)
- [ğŸ“Š Results](#-results)
- [ğŸš€ Usage](#-usage)
- [ğŸ‘¤ Author](#-author)
- [ğŸ“œ License](#-license)

---

## ğŸ“– Overview

- **Model Used:** `distilbert-base-uncased`
- **Task:** Binary classification (Fake News vs. Real News)
- **Dataset:** CSV file containing labeled news articles
- **Key Steps:**
  - Load dataset using Pandas ğŸ“Š
  - Tokenize text with DistilBERT tokenizer âœï¸
  - Fine-tune the model on the dataset âš¡
  - Evaluate performance using accuracy metrics ğŸ“ˆ
- **Final Accuracy:** **99.7%** ğŸ¯

---

## ğŸ› ï¸ Setup & Installation

To use this project, first clone the repository:

```bash
git clone https://github.com/harshhmaniya/Fake-News-Classification-DistilBERT-99.7-accuracy.git
cd Fake-News-Classification-DistilBERT-99.7-accuracy
```

### Install Dependencies

Ensure that the required libraries are installed. If a `requirements.txt` file is not present, you can manually install the necessary packages:

```bash
pip install transformers pandas numpy torch scikit-learn
```

---

## ğŸ“‚ Dataset

The dataset consists of **news articles** labeled as either **real** or **fake**. The data is loaded from a **CSV file** using Pandas. Before training, the **DistilBERT tokenizer** is used to process the text.

---

## âš™ï¸ Model Training Process

The Jupyter Notebook guides through the following steps:

1. **Data Preprocessing:**
   - Load the dataset using Pandas.
   - Tokenize text data using the **DistilBERT tokenizer**.

2. **Fine-Tuning DistilBERT:**
   - Load the pre-trained model (`distilbert-base-uncased`).
   - Train the model for **3 epochs** with optimized hyperparameters.

3. **Evaluation:**
   - Compute accuracy and performance metrics.
   - **Final Accuracy:** **99.7%** on the test set.

4. **Saving the Fine-Tuned Model:**
   - The model and tokenizer are saved for future inference.

---

## ğŸ“Š Results

- **Test Accuracy:** **99.7%** âœ…
- **Evaluation Metrics:** Detailed in the notebook.
- **Performance Visualization:** Training loss graphs and accuracy plots included.

---

## ğŸš€ Usage

Once trained, you can use the fine-tuned model to classify new news articles.

### Load the Fine-Tuned Model

```python
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification

# Load tokenizer and trained model (update path accordingly)
tokenizer = DistilBertTokenizer.from_pretrained('path_to_saved_tokenizer')
model = DistilBertForSequenceClassification.from_pretrained('path_to_saved_model')
```

### Make Predictions

```python
text = "Breaking news: Scientists discover water on Mars!"
inputs = tokenizer(text, return_tensors="pt")
outputs = model(**inputs)
prediction = outputs.logits.argmax(dim=1).item()

print("Fake News" if prediction == 1 else "Real News")
```

ğŸ”¹ **Tip:** Adjust the model path according to where it's saved.

---

## ğŸ‘¤ Author

**Harsh Maniya**

ğŸ”— [Hugging Face](https://huggingface.co/harshhmaniya)

---

## ğŸ“œ License

This project is licensed under the **MIT License**.

---

âœ¨ Enjoy exploring and improving this project! ğŸš€

*Note: For detailed code and further information, refer to the [Jupyter Notebook](https://github.com/harshhmaniya/Fake-News-Classification-DistilBERT-99.7-accuracy/blob/main/real_fake.ipynb) included in this repository.* 
